#+title: Use LLM to parse MESA options

[[https://docs.mesastar.org/en/latest/][MESA]] is an open-source and community-driven stellar evolution code,
which has thousands of options that can be combined into an extremely
large number of setups for different purposes. While most combinations
are probably non-physical nonsense, it can be hard to navigate all the
existing options, and it is an art to find what works for your science
case. Large language models (LLM) are very good at parsing code and
text (e.g., the documentation of the code!).

This is an attempt to use [[https://docs.llamaindex.ai/en/stable/][llama-index]] to create an helper to navigate
MESA options. Testing with =MESA r24.03.1= and =llama3=, which requires at
least 32GB of RAM memory to run locally.


** Installation

1. You should [[https://docs.mesastar.org/en/latest/installation.html][install MESA]] and export its location as =MESA_DIR=. For
   now this tries to use for context all the =*.f90=, =*.f=, =*.defaults=,
   =*.list=, =*.inc=, and =*.dek= files recursively found in =MESA_DIR=.
2. Run =conda env create -f environment.yml= to setup the required python environment
3. Install [[https://github.com/ollama/ollama][ollama]] following the README there. In short, run =curl -fsSL https://ollama.com/install.sh | sh=
   Note that this will require =sudo= privileges.
4. Install =llama3= as the LLM we will use locally with =ollama pull llama3=.
5. Install nomic embedding =ollama pull nomic-embed-text=.

** Usage

(This ideally will be streamlined into a chatbot once a LLM capable of
delivering exists)

1. If not done already, activate the environment with =conda activate LLM=
2. Start a notebook by running =ipython=. Within the notebook run the
   script [[./test.py][test.py]] with =%run ./test.py=. This will initialize things,
   most importantly the =query_engine= and show a template on how to use it.
3. From within the notebook, create questions and print them (as per
   the template shown by =test.py=), e.g.:

#+begin_src python
In [2]: response = query_engine.query("What is dq?")

In [3]: print(response)
Based on the provided context information, there is no mention of a variable or parameter named "dq". Therefore, I cannot provide an answer to the query. The given files contain various parameters and settings for controlling simulations using the LLMESA code, but none of them seem to relate to a variable named "dq".
#+end_src

*N.B.:* Obviously this is not true, =dq= is defined in =$MESA_DIR/star/defaults/profile_columns.list=. Looking into improving this.
