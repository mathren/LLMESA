#+title: Use LLM to parse MESA options

[[https://docs.mesastar.org/en/latest/][MESA]] is an open-source and community-driven stellar evolution code,
which has thousands of options that can be combined into an extremely
large number of setups for different purposes.

While most combinations are probably non-physical nonsense, it can be
hard to navigate all the existing options, and it is an art to find
what works for your science case.

Large language models (LLM) are very good at parsing code and text
(e.g., the documentation of the code).

This is an attempt to use [[https://docs.llamaindex.ai/en/stable/][llama-index]] to create an helper to navigate
MESA options.

** Steps necessary

1. Make sure you have enough RAM (32GB recommended)
2. You should [[https://docs.mesastar.org/en/latest/installation.html][install MESA]] and export its location as =MESA_DIR=.
3. Run =conda env create -f environment.yml= to setup the required
   python environment
4. Install [[https://github.com/ollama/ollama][ollama]] following the README there, in short, run =curl -fsSL https://ollama.com/install.sh | sh=
   Note that this will require =sudo= privileges
5. Install =llama3= as the LLM we will use locally with =ollama pull
   llama3=.
6. Install nomic embedding =ollama pull nomic-embed-text=
